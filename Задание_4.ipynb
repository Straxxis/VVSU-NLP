{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, activation_hidden='relu', activation_output='sigmoid', task='classification', learning_rate=0.01):\n",
        "        np.random.seed(42)\n",
        "        self.lr = learning_rate\n",
        "        self.task = task\n",
        "        self.activation_hidden = activation_hidden\n",
        "        self.activation_output = activation_output\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        sizes = [input_size] + hidden_sizes + [output_size]\n",
        "        for i in range(len(sizes) - 1):\n",
        "            self.weights.append(np.random.randn(sizes[i], sizes[i+1]) * np.sqrt(2 / sizes[i]))\n",
        "            self.biases.append(np.zeros((1, sizes[i+1])))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.activations = [X]\n",
        "        self.z_values = []\n",
        "        for i in range(len(self.weights)):\n",
        "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
        "            self.z_values.append(z)\n",
        "            is_output_layer = (i == len(self.weights) - 1)\n",
        "            if is_output_layer:\n",
        "                if self.task == 'regression':\n",
        "                    activation = z  # linear\n",
        "                elif self.activation_output == 'softmax':\n",
        "                    activation = self.softmax(z)\n",
        "                elif self.activation_output == 'sigmoid':\n",
        "                    activation = self.sigmoid(z)\n",
        "                else:\n",
        "                    activation = z  # no activation\n",
        "            else:\n",
        "                if self.activation_hidden == 'relu':\n",
        "                    activation = self.relu(z)\n",
        "                else:\n",
        "                    activation = self.sigmoid(z)\n",
        "            self.activations.append(activation)\n",
        "        return self.activations[-1]\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        errors = []\n",
        "        deltas = []\n",
        "        m = X.shape[0]\n",
        "\n",
        "        if self.task == 'classification':\n",
        "            # Кросс-энтропия с softmax или MSE + sigmoid\n",
        "            if self.activation_output == 'softmax':\n",
        "                delta = (output - y) / m  # cross-entropy gradient for softmax + onehot\n",
        "            else:\n",
        "                error = y - output\n",
        "                delta = error * self.sigmoid_derivative(output)\n",
        "        else:  # regression with MSE and linear output\n",
        "            error = (output - y) / m\n",
        "            delta = error\n",
        "\n",
        "        deltas.append(delta)\n",
        "\n",
        "        for i in range(len(self.weights) - 2, -1, -1):\n",
        "            if self.activation_hidden == 'relu':\n",
        "                deriv = self.relu_derivative(self.activations[i+1])\n",
        "            else:\n",
        "                deriv = self.sigmoid_derivative(self.activations[i+1])\n",
        "            delta = deltas[-1].dot(self.weights[i+1].T) * deriv\n",
        "            deltas.append(delta)\n",
        "\n",
        "        deltas.reverse()\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= self.lr * self.activations[i].T.dot(deltas[i])\n",
        "            self.biases[i] -= self.lr * np.sum(deltas[i], axis=0, keepdims=True)\n",
        "\n",
        "    def train(self, X, y, epochs=2000, batch_size=None):\n",
        "        for epoch in range(epochs):\n",
        "            if batch_size is None:\n",
        "                output = self.forward(X)\n",
        "                self.backward(X, y, output)\n",
        "            else:\n",
        "                permutation = np.random.permutation(X.shape[0])\n",
        "                X_shuffled = X[permutation]\n",
        "                y_shuffled = y[permutation]\n",
        "                for start in range(0, X.shape[0], batch_size):\n",
        "                    end = start + batch_size\n",
        "                    xb = X_shuffled[start:end]\n",
        "                    yb = y_shuffled[start:end]\n",
        "                    output = self.forward(xb)\n",
        "                    self.backward(xb, yb, output)\n",
        "\n",
        "            if epoch % 100 == 0 or epoch == epochs - 1:\n",
        "                output_full = self.forward(X)\n",
        "                loss = np.mean(np.square(y - output_full))\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "# --- Классификация iris ---\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target.reshape(-1, 1)\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_iris = encoder.fit_transform(y_iris)\n",
        "\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)\n",
        "\n",
        "# Нормализация по всему набору, чтобы избежать утечки информации из теста\n",
        "X_mean = np.mean(X_iris, axis=0)\n",
        "X_std = np.std(X_iris, axis=0)\n",
        "X_train_iris = (X_train_iris - X_mean) / X_std\n",
        "X_test_iris = (X_test_iris - X_mean) / X_std\n",
        "\n",
        "nn_classification = NeuralNetwork(\n",
        "    input_size=4,\n",
        "    hidden_sizes=[10, 10],\n",
        "    output_size=3,\n",
        "    activation_hidden='relu',\n",
        "    activation_output='softmax',\n",
        "    task='classification',\n",
        "    learning_rate=0.01\n",
        ")\n",
        "nn_classification.train(X_train_iris, y_train_iris, epochs=2000, batch_size=16)\n",
        "\n",
        "predictions_iris = nn_classification.predict(X_test_iris)\n",
        "predicted_classes = np.argmax(predictions_iris, axis=1)\n",
        "true_classes = np.argmax(y_test_iris, axis=1)\n",
        "accuracy = np.mean(predicted_classes == true_classes)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# --- Регрессия на случайных данных ---\n",
        "data = np.random.rand(1000, 4)\n",
        "columns = ['Joke1', 'Joke2', 'Joke3', 'Anekdot']\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "X_reg = df[['Joke1', 'Joke2', 'Joke3']].values\n",
        "y_reg = df['Anekdot'].values.reshape(-1, 1)\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_reg = scaler_X.fit_transform(X_reg)\n",
        "y_reg = scaler_y.fit_transform(y_reg)\n",
        "\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "hidden_neurons = int(input(\"Введите количество нейронов \"))\n",
        "epochs = int(input(\"Введите количество обучения: \"))\n",
        "\n",
        "nn_regression = NeuralNetwork(\n",
        "    input_size=3,\n",
        "    hidden_sizes=[hidden_neurons],\n",
        "    output_size=1,\n",
        "    activation_hidden='relu',\n",
        "    activation_output='linear',\n",
        "    task='regression',\n",
        "    learning_rate=0.01\n",
        ")\n",
        "nn_regression.train(X_train_reg, y_train_reg, epochs=epochs, batch_size=16)\n",
        "\n",
        "y_pred = nn_regression.predict(X_test_reg)\n",
        "y_pred = scaler_y.inverse_transform(y_pred)\n",
        "y_test_orig = scaler_y.inverse_transform(y_test_reg)\n",
        "\n",
        "print(\"Real vs Predicted:\")\n",
        "for real, pred in zip(y_test_orig[:100], y_pred[:100]):\n",
        "    print(f\"Real: {real[0]:.2f}, Predicted: {pred[0]:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JFgcZb5OZcM",
        "outputId": "fd61034f-4ccc-4949-f166-ba5b3a9494ce"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2587\n",
            "Epoch 100, Loss: 0.0423\n",
            "Epoch 200, Loss: 0.0151\n",
            "Epoch 300, Loss: 0.0115\n",
            "Epoch 400, Loss: 0.0102\n",
            "Epoch 500, Loss: 0.0099\n",
            "Epoch 600, Loss: 0.0094\n",
            "Epoch 700, Loss: 0.0092\n",
            "Epoch 800, Loss: 0.0091\n",
            "Epoch 900, Loss: 0.0088\n",
            "Epoch 1000, Loss: 0.0087\n",
            "Epoch 1100, Loss: 0.0086\n",
            "Epoch 1200, Loss: 0.0086\n",
            "Epoch 1300, Loss: 0.0085\n",
            "Epoch 1400, Loss: 0.0084\n",
            "Epoch 1500, Loss: 0.0084\n",
            "Epoch 1600, Loss: 0.0084\n",
            "Epoch 1700, Loss: 0.0084\n",
            "Epoch 1800, Loss: 0.0083\n",
            "Epoch 1900, Loss: 0.0083\n",
            "Epoch 1999, Loss: 0.0082\n",
            "Test Accuracy: 100.00%\n",
            "Введите количество нейронов 10\n",
            "Введите количество обучения: 1\n",
            "Epoch 0, Loss: 1.0690\n",
            "Real vs Predicted:\n",
            "Real: 0.60, Predicted: 0.54\n",
            "Real: 0.06, Predicted: 0.40\n",
            "Real: 0.03, Predicted: 0.44\n",
            "Real: 0.08, Predicted: 0.40\n",
            "Real: 0.20, Predicted: 0.56\n",
            "Real: 0.26, Predicted: 0.55\n",
            "Real: 0.92, Predicted: 0.33\n",
            "Real: 0.33, Predicted: 0.55\n",
            "Real: 0.53, Predicted: 0.47\n",
            "Real: 0.17, Predicted: 0.55\n",
            "Real: 0.57, Predicted: 0.58\n",
            "Real: 0.83, Predicted: 0.47\n",
            "Real: 0.31, Predicted: 0.55\n",
            "Real: 0.13, Predicted: 0.48\n",
            "Real: 0.61, Predicted: 0.31\n",
            "Real: 0.11, Predicted: 0.55\n",
            "Real: 0.86, Predicted: 0.39\n",
            "Real: 0.85, Predicted: 0.54\n",
            "Real: 0.68, Predicted: 0.54\n",
            "Real: 0.92, Predicted: 0.50\n",
            "Real: 0.50, Predicted: 0.48\n",
            "Real: 0.48, Predicted: 0.50\n",
            "Real: 0.07, Predicted: 0.56\n",
            "Real: 0.06, Predicted: 0.57\n",
            "Real: 0.90, Predicted: 0.55\n",
            "Real: 0.63, Predicted: 0.47\n",
            "Real: 0.04, Predicted: 0.31\n",
            "Real: 0.48, Predicted: 0.46\n",
            "Real: 0.79, Predicted: 0.51\n",
            "Real: 0.27, Predicted: 0.49\n",
            "Real: 0.49, Predicted: 0.50\n",
            "Real: 0.93, Predicted: 0.57\n",
            "Real: 0.71, Predicted: 0.56\n",
            "Real: 0.72, Predicted: 0.54\n",
            "Real: 0.36, Predicted: 0.25\n",
            "Real: 0.62, Predicted: 0.37\n",
            "Real: 0.91, Predicted: 0.48\n",
            "Real: 0.26, Predicted: 0.50\n",
            "Real: 0.89, Predicted: 0.56\n",
            "Real: 0.01, Predicted: 0.49\n",
            "Real: 0.64, Predicted: 0.55\n",
            "Real: 0.77, Predicted: 0.57\n",
            "Real: 0.15, Predicted: 0.55\n",
            "Real: 0.38, Predicted: 0.56\n",
            "Real: 0.02, Predicted: 0.43\n",
            "Real: 0.66, Predicted: 0.45\n",
            "Real: 0.79, Predicted: 0.55\n",
            "Real: 0.48, Predicted: 0.50\n",
            "Real: 0.32, Predicted: 0.41\n",
            "Real: 0.69, Predicted: 0.37\n",
            "Real: 0.28, Predicted: 0.57\n",
            "Real: 0.55, Predicted: 0.38\n",
            "Real: 0.70, Predicted: 0.50\n",
            "Real: 0.85, Predicted: 0.59\n",
            "Real: 0.35, Predicted: 0.54\n",
            "Real: 0.87, Predicted: 0.55\n",
            "Real: 0.18, Predicted: 0.55\n",
            "Real: 0.76, Predicted: 0.55\n",
            "Real: 0.23, Predicted: 0.55\n",
            "Real: 0.73, Predicted: 0.40\n",
            "Real: 0.48, Predicted: 0.33\n",
            "Real: 0.95, Predicted: 0.36\n",
            "Real: 0.80, Predicted: 0.44\n",
            "Real: 0.80, Predicted: 0.57\n",
            "Real: 0.43, Predicted: 0.44\n",
            "Real: 0.11, Predicted: 0.39\n",
            "Real: 0.97, Predicted: 0.55\n",
            "Real: 0.96, Predicted: 0.47\n",
            "Real: 0.71, Predicted: 0.37\n",
            "Real: 0.13, Predicted: 0.57\n",
            "Real: 0.37, Predicted: 0.37\n",
            "Real: 0.23, Predicted: 0.51\n",
            "Real: 0.72, Predicted: 0.39\n",
            "Real: 0.28, Predicted: 0.57\n",
            "Real: 0.11, Predicted: 0.53\n",
            "Real: 0.45, Predicted: 0.51\n",
            "Real: 0.94, Predicted: 0.49\n",
            "Real: 0.91, Predicted: 0.28\n",
            "Real: 0.69, Predicted: 0.48\n",
            "Real: 0.83, Predicted: 0.24\n",
            "Real: 0.46, Predicted: 0.55\n",
            "Real: 0.88, Predicted: 0.50\n",
            "Real: 0.66, Predicted: 0.55\n",
            "Real: 0.45, Predicted: 0.55\n",
            "Real: 0.93, Predicted: 0.50\n",
            "Real: 0.88, Predicted: 0.30\n",
            "Real: 0.87, Predicted: 0.52\n",
            "Real: 0.17, Predicted: 0.57\n",
            "Real: 0.15, Predicted: 0.53\n",
            "Real: 0.15, Predicted: 0.50\n",
            "Real: 0.05, Predicted: 0.54\n",
            "Real: 0.84, Predicted: 0.55\n",
            "Real: 0.83, Predicted: 0.52\n",
            "Real: 0.22, Predicted: 0.54\n",
            "Real: 0.93, Predicted: 0.44\n",
            "Real: 0.62, Predicted: 0.55\n",
            "Real: 0.53, Predicted: 0.50\n",
            "Real: 0.66, Predicted: 0.52\n",
            "Real: 0.49, Predicted: 0.54\n",
            "Real: 0.13, Predicted: 0.56\n"
          ]
        }
      ]
    }
  ]
}